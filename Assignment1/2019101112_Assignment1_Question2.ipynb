{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866ed254",
   "metadata": {},
   "source": [
    "# Approximate Nearest Neighbors:\n",
    "\n",
    "# Image Recommendation System via Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08937a",
   "metadata": {},
   "source": [
    "# ***Please read the instructions very carefully***\n",
    "This is a modified version of the previous question and requires you to use an artificial nearest neighbors library\n",
    "\n",
    "We suggest you to use one of the following:\n",
    "- [ScaNN](https://github.com/google-research/google-research/tree/master/scann)\n",
    "- [FAISS](https://github.com/facebookresearch/faiss)\n",
    "- [Annoy](https://github.com/spotify/annoy.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a4212",
   "metadata": {},
   "source": [
    "1.   Assignment must be implemented in Python 3 only.\n",
    "2.   You are allowed to use libraries for data preprocessing (numpy, pandas, nltk etc) and for evaluation metrics, data visualization (matplotlib etc.).\n",
    "3.   You will be evaluated not just on the overall performance of the model and also on the experimentation with hyper parameters, data prepossessing techniques etc.\n",
    "4.   ⚠️ The Assignment will be evaluated automatically. Please adhere to taking proper inputs from `config.csv` file. You can change your `config.csv` file to experiment with your code. But at the end, make sure that your outputs are corresponding to input values in `config.csv`\n",
    "5.   Strict plagiarism checking will be done. An F will be awarded for plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494560c",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "Behance is a community art website where users showcase and discover creative work. Each user is able to “appreciate” (equivalent to a “like” on Instagram or a “react” on Facebook) an image, indicating that they like the image. It is in the website’s best interests to show users pictures that they would like, to keep them engaged for longer. For this question, given a set of pictures that a user has already appreciated, you have to show them a new picture that they would like based on what similar users appreciated.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "**The dataset has information of 1 million appreciates of 63,497 users on 178,788 items. The file Behance appreciate 1M has a triplet in each line in the form of (user id, item id, unix timestamp).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd3f15",
   "metadata": {},
   "source": [
    "**Task: Take the inputs from the config.csv file and output the recommendations for a particular person**\n",
    "- Collaborative Filtering is a way to predict items to the user based on the the\n",
    "user’s history and the history of similar users. The similarity between users can be quantified by the number of images that both the users appreciated.\n",
    "- The images appreciated by a similar user would be the most suitable images to show a user. Since we can find the similarity between any two users, we would be able to find the “nearest” neighbours of any user, allowing us to use a KNN-based algorithm to recommend new images to a user.\n",
    "- Since people do not like seeing pictures that they have seen already. Make sure that you do not recommend pictures that a user has appreciated already.\n",
    "- Output the final response will be saved in the file named ```config['output_file']```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a52fe5",
   "metadata": {},
   "source": [
    "**Output file format:**\n",
    "Populate the output file with images that the user has not seen of the k most\n",
    "similar users, in descending order of their similarity. Each line in the output\n",
    "file should be a duplet in the form of (item id, user id), where the user id is the\n",
    "id of the kth similar user. The order of the images corresponding to the same\n",
    "similar user would not matter. The output file would look something like this:\n",
    "```\n",
    "item_id_1_of_1st_similar_user 1st_most_similar_user_id\n",
    "item_id_2_of_1st_similar_user 1st_most_similar_user_id\n",
    "item_id_3_of_1st_similar_user 1st_most_similar_user_id\n",
    "...\n",
    "item_id_1_of_2nd_similar_user 2nd_most_similar_user_id\n",
    "item_id_2_of_2nd_similar_user 2nd_most_similar_user_id\n",
    "item_id_3_of_2nd_similar_user 2nd_most_similar_user_id\n",
    "...\n",
    "item_id_1_of_kth_similar_user kth_most_similar_user_id\n",
    "item_id_2_of_kth_similar_user kth_most_similar_user_id\n",
    "item_id_3_of_kth_similar_user kth_most_similar_user_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d46fd2",
   "metadata": {},
   "source": [
    "You may use any other recommendation system that you wish to use. However,\n",
    "evaluation script will score your submission by measuring the similarity between\n",
    "users with the number of common images they appreciated.\n",
    "The dataset was extracted using Behance’s API as a part of the paper\n",
    "“Vista: A visually, socially, and temporally-aware model for artistic\n",
    "recommendation, RecSys, 2016”. Check out this [Google Drive folder](https://drive.google.com/drive/folders/0B9Ck8jw-TZUEc3NlMjVXdDlPU1k?resourcekey=0-6_8ykn0o4fLc5fuTEm91xA) for\n",
    "more information about the dataset.\n",
    "\n",
    "\n",
    "Have fun! The users are waiting to see new pictures!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912088cf",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e9cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9c4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Generation Sample Code.\n",
    "# ⚠️ Only for experimentation on your side.\n",
    "# ⚠️ Should be commented during submission.\n",
    "\n",
    "# df = pd.DataFrame(data=[{'id':276633,\n",
    "#                   'k':5,\n",
    "#                   'dataset_file':'./Behance_appreciate_1M',\n",
    "#                   'output_file':'./output.txt'}])\n",
    "# df.to_csv('config.csv')\n",
    "\n",
    "# df = pd.DataFrame(data=[{'id':10356,\n",
    "#                   'k':5,\n",
    "#                   'dataset_file':'./Behance_subsampled.txt',\n",
    "#                   'output_file':'./output2.txt'}])\n",
    "# df.to_csv('config.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb6bc1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pd.read_csv('config.csv').iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef721d6b",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Unnamed: 0                             0\n",
       "id                                 10356\n",
       "k                                      5\n",
       "dataset_file    ./Behance_subsampled.txt\n",
       "output_file                ./output2.txt\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4966a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = config['id']\n",
    "k_value = config['k']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e50dee",
   "metadata": {},
   "source": [
    "### Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd74173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['dataset_file'], 'r') as inFile:\n",
    "    appreciate_data = inFile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8542cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e9961",
   "metadata": {},
   "source": [
    "### Initialize a dictionary to store the item_ids that a user likes\n",
    "\n",
    "### Go through each line of the input file and construct the user_likes dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff4a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_likes = dict()\n",
    "items = set()\n",
    "users = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b55dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in appreciate_data:\n",
    "    line = line.strip()\n",
    "    \n",
    "    user_id = int(line.split()[0])\n",
    "    item_id = int(line.split()[1])\n",
    "\n",
    "    items.add(item_id)\n",
    "    users.add(user_id)\n",
    "\n",
    "    if user_id not in user_likes:\n",
    "        user_likes[user_id] = list()\n",
    "    \n",
    "    user_likes[user_id].append(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10356"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "items = sorted(items)\n",
    "items[0]\n",
    "users = sorted(users)\n",
    "users[0]\n",
    "# users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_likes_matrix = [ [0 for i in range(len(items))] for i in range(len(list(user_likes.keys()))) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_idx in range(len(user_likes.keys())):\n",
    "    for item_idx in range(len(user_likes_matrix[user_idx])):\n",
    "        if items[item_idx] in user_likes[users[user_idx]]:\n",
    "            user_likes_matrix[user_idx][item_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting latent features\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def get_latent_features(user_likes_matrix):\n",
    "    epsilon = 1e-9\n",
    "    n_latent_factors = 1000\n",
    "    user_svd = TruncatedSVD(n_components = n_latent_factors)\n",
    "    user_features = user_svd.fit_transform(user_likes_matrix) + epsilon\n",
    "\n",
    "    return user_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = get_latent_features(user_likes_matrix).astype(\"float32\")\n",
    "# type(user_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd017880",
   "metadata": {},
   "source": [
    "### Use your choice of Approximate Nearest Neigbor after Collaborative Filtering to find nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss                   # make faiss available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = 0\n",
    "for i in range(len(users)):\n",
    "    if users[i] == user: \n",
    "        user_idx = i\n",
    "        break\n",
    "query_features = [user_features[user_idx]]\n",
    "query_features = np.array(query_features).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ee1d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def neighbors(user,user_features,query_features,k_value):\n",
    "    \"\"\" returns an iterable object (like list or generator) \"\"\"\n",
    "\n",
    "    nlist = 5  # number of clusters\n",
    "    quantiser = faiss.IndexFlatL2(1000)  \n",
    "    index = faiss.IndexIVFFlat(quantiser, 1000, nlist,  faiss.METRIC_L2)\n",
    "    # print(index.is_trained)\n",
    "    index.train(user_features)\n",
    "    # print(index.ntotal)\n",
    "    index.add(user_features)                  # add vectors to the index\n",
    "    # print(index.is_trained)\n",
    "    # print(index.ntotal)\n",
    "\n",
    "    index.nprobe = 2\n",
    "    k = int(k_value + 1)                      # we want to see k+1 nearest neighbors\n",
    "    D, I = index.search(query_features, k=k)     # actual search\n",
    "    # print(I)                   # neighbors of the 5 first queries\n",
    "\n",
    "    neighbors = []\n",
    "    for user_idx in I[0]:\n",
    "        # print(users[user_idx])\n",
    "        if users[user_idx] == user: continue\n",
    "        neighbors.append(users[user_idx])\n",
    "\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142440b0",
   "metadata": {},
   "source": [
    "### Open the output file to write all the lines to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "977b08f9",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n0\nTrue\n1000\n"
     ]
    }
   ],
   "source": [
    "outFile = open(config['output_file'], 'w')\n",
    "\n",
    "for n_user in neighbors(user,user_features,query_features,k_value):\n",
    "    user_id = n_user\n",
    "    for item_id in user_likes[user_id]:\n",
    "        if item_id not in user_likes[user]:\n",
    "            outFile.write(str(item_id) + ' ' + str(user_id) + '\\n')\n",
    "\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f24d6",
   "metadata": {},
   "source": [
    "### Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f72f8",
   "metadata": {},
   "source": [
    "#### Q1. **Explain how your choice of library works**\n",
    "\n",
    "Chosen library: FAISS\n",
    "\n",
    "FAISS is a library developed by Facebook AI research that contains algorithms for efficient similarity search for vectors of any size, even those that might not fit in RAM. FAISS compares vectors with Euclidean (l2) distances and looks for vectors with the lowest l2 distance as the given query vector.\n",
    "\n",
    "It is build around an index class that stores the set of vectors which the distance of which will be compared to the query vector. It provides a \"search\" function to search in this index with l2 vector comparision. FAISS essentially builds the index data structure in RAM and can then efficiently perform the operation: \n",
    "\n",
    "i = argmin_i ||x - x_i||\n",
    "\n",
    "where there are a given set of vectors x_i of dimension d, and a new vector x is given. The search function is the computation of this argmin. There are different types of index structures which have their own tradeoffs with respect to search time, quality, memory used, and trainin time.\n",
    "\n",
    "Using the distance metric, faiss first builds the Approximate Nearest Neighbours graph of the vectors. To make the search efficient, Faiss uses approximate non-exhaustive search on a Nearest-Neighbours graph. It also transforms the vectors to have a lower dimensionality. This preprocessing step in our case was to L2 normalize the vectors. The next step is inverted file indexing which partitions the vectors into similar clusters/centroids. So when our query vector is searched , it first calculates distance to nearest centroid of a cluster and only then the vectors of that cluster are accessed. This is followed by a last vector encoding step to further make index efficient. In all , faiss offers approximate search for a tiny reduction in accuracy for a very good performance in time and memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ac14b",
   "metadata": {},
   "source": [
    "#### Q2. **Compare your choice of library with vanilla KNN.**\n",
    "***Hint: Include Time Complexity, and explain the tradeoff with recall***\n",
    "\n",
    "FAISS can perform vanilla KNN similar to what we have implemented in question 1 using the IndexFlatL2 data structure. In order to employ Approximate Nearest Neighbors as this question asked, we use the IndexIVFFlat data structure which takes a list in vectors that represent the database and performs Voronoi Tesselation in the n-dimensional vector space. This type of index requires a quantizer which is another index data structure (IndexFlatL2) which assigns vectors to Voronoi cells. Each cell is defined by centroid and by finding the nearest neighbour of the vector in the set of centroids the voronoi cell is found. During search, we can define how many neighbouring cells to probe for nearest neighbours from the nprobe parameter. This implementation leads to a much faster search speed. As per documentation, using approximation leads to a 10x increase in search speed with a 10% recall reduction.\n",
    "\n",
    "The time complexity of vanilla KNN is O(n) for each query vector, n being the size of the dataset.\n",
    "\n",
    "The library also allows ways to optimize the vector storage using another quantizer that is based on lossy compression techniues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d7827",
   "metadata": {},
   "source": [
    "#### Q3. **Compare your choice of library with implementation of ScaNN, faiss and annoy.**\n",
    "***Hint: Include Time Complexity, and explain the tradeoff with recall***\n",
    "\n",
    "ScaNN: Scalable Nearest Neighbours is a library from Google Research that efficiently searched for similar vectors at scale. It follows the following steps :\n",
    "    * Partitions the dataset while training to get the top partition in order to use during the scoring stage at query time. Implemented via a tree data structure. More leaves leads to higher quality of partioning with the drawback of longer time taken\n",
    "    * Computing the distance from query vector to all datapoints in a partition while searching. Done either brute force which has the best accuracy or asymmetric hashing which works faster but leads to a drop in accuracy. There is also a quantized brute force that quantizes each dimensions datapoint. With large datasets this works much faster than brute force and doesn't have much change in accuracy\n",
    "    * Rescoring the best k vector distances and recomputes the distances more accurately, then choosing top k\n",
    "\n",
    "FAISS: supports differnt KNN algorithms with their own speed, memory, accuracy tradeoffs such as brute force, voronoi tesselation, product quantization etc by using a different index data structure\n",
    "\n",
    "Annoy: implemented with a tree data structure. At each node in the tree, a random hyperplane is chosen which divides the space into two subspaces. A hyperplace is chosen by taking to points from a subset and taking the equidistant hyperplane from them. After doing this k times, you get a forest of trees. Depending on k, there is a precision-performance tradeoff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}